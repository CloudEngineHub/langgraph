{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83bf198-9d20-447c-9565-123345dd90a9",
   "metadata": {},
   "source": [
    "# Add Long-term User Memory\n",
    "\n",
    "LangGraph already provides graph persistence as a first-class feature. That's all you need to have **thread** level memory (aka chat history memory).\n",
    "\n",
    "There are huge benefits to persisting memory **across threads**, however. This notebook shows how to implement a simple 'User Profile' type memory as an async process and connect it to your LangGraph.\n",
    "\n",
    "The user profile can be any schema. We will naively overwrite the user profile any time a new thread is scheduled to process memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19614a-fb58-49d0-8fec-dcacc985fc75",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's install this project's prereqs. We will use Claude for everything. Feel free to swap it out for any model that can reasonably perform function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e637a885-c557-43f1-8059-70d6d2ac7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph aiosqlite langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c16e7dc-66bf-4c78-af70-ae07aba93f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_PROJECT=langgraph-long-term-memory\n"
     ]
    }
   ],
   "source": [
    "%env LANGCHAIN_PROJECT=langgraph-long-term-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f855-d660-4ef3-a9d7-00fa4d42d2dd",
   "metadata": {},
   "source": [
    "## Memory DB\n",
    "\n",
    "First, we will set up a table in our database to store user memories. For this how-to, we will use `sqlite` (since it requires little additional setup), but you can swap this out with postgres or whatever other database you'd like.\n",
    "We will re-use this DB for our graph checkpointing.\n",
    "\n",
    "First, create the memories table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce108ac-9ee7-4712-a7bd-a3d1d7796ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiosqlite\n",
    "\n",
    "conn_string = \":memory:\"\n",
    "conn = aiosqlite.connect(conn_string)\n",
    "await conn\n",
    "async with conn.executescript(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS core_memories (\n",
    "        user_id TEXT NOT NULL,\n",
    "        memory TEXT NOT NULL,\n",
    "        PRIMARY KEY (user_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "):\n",
    "    await conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bb7ca-527d-41ce-a32a-6b97c49b26a7",
   "metadata": {},
   "source": [
    "Next, define the accessor methods. These just upsert or get the memory for a specific user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168dbcbd-545c-4579-bfa3-cea67e56879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "async def get_user_profile(conn, user_id):\n",
    "    async with conn.execute(\n",
    "        \"SELECT memory FROM core_memories WHERE user_id = ?\",\n",
    "        (user_id,),\n",
    "    ) as cursor:\n",
    "        if value := await cursor.fetchone():\n",
    "            memory_str = value[0]\n",
    "            return json.loads(memory_str)\n",
    "        return None\n",
    "\n",
    "\n",
    "async def commit_user_profile(conn, user_id, profile):\n",
    "    async with conn.execute(\n",
    "        \"INSERT OR REPLACE INTO core_memories (user_id, memory) VALUES (?, ?)\",\n",
    "        (\n",
    "            user_id,\n",
    "            profile.json(),\n",
    "        ),\n",
    "    ):\n",
    "        await conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d50e9-c272-4e74-8539-413ebfc0d991",
   "metadata": {},
   "source": [
    "Next, define the LLM to extract the user profile from threads. Feel free to customize this step! The key components are:\n",
    "\n",
    "1. The memory schema to populate. We have a very simple schema that contains a list of core memories.\n",
    "2. Formatted messages to ensure the LLM extracts memories about the user (rather than the assistant or other users)\n",
    "3. Handling to load and save the memories to the DB.\n",
    "\n",
    "Note that here we are naively re-generating the state on each thread invocation. We have found better results if we do a JSONPatch schema to perform updates (after the initial generation) as that requires less work on the LLM's behalf and reduces unwanted deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6c9498-66b5-4323-88f8-1e0146e11614",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatAnthropic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully committed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# TODO: Add the retries + persistence. We got some fun tricks up our sleeve for extraction improvements\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m mem_llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatAnthropic\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-sonnet-20240229\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m mem_chain \u001b[38;5;241m=\u001b[39m prepare_inputs \u001b[38;5;241m|\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(extracted\u001b[38;5;241m=\u001b[39mprompt \u001b[38;5;241m|\u001b[39m mem_llm\u001b[38;5;241m.\u001b[39mwith_structured_output(UserProfile)) \u001b[38;5;241m|\u001b[39m commit_extraction\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatAnthropic' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    core_memories: list[str] = Field(\n",
    "        ..., description=\"All core memories from this conversation.\"\n",
    "    )\n",
    "    interests: list[str] = Field(\n",
    "        ...,\n",
    "        descriptions=\"Interests the user has expressed, like specific sports, hobbies, beliefs, etc.\",\n",
    "    )\n",
    "    name: list[str] = Field(..., description=\"The user's name (if shared)\")\n",
    "    age: Optional[int] = Field(default=None, description=\"The user's age (if shared)\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\"Below, you are given one or more conversations between {user_id} and an AI.\n",
    "\n",
    "Use the provided function to save all salient information about user {user_id}.\n",
    "Refrain from recording information about the AI or other users that is not directly relevant to user {user_id}.{current_user_state}\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"<moderator>Reflect on the above conversation and update the user profile based on {user_id}'s revelations.</moderator>\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "_CURRENT_STATE_TEMPLATE = \"\"\"\n",
    "## Current User Profile\n",
    "<profile>\n",
    "{current_user_state}\n",
    "</profile>\n",
    "\n",
    "Your response will overwrite this profile, so please ensure to retain all information you don't\n",
    "wish to lose. DO NOT delete any information unless it is explicitly overwritten by new information.\"\"\"\n",
    "\n",
    "\n",
    "async def prepare_inputs(inputs: dict):\n",
    "    messages = inputs[\"messages\"]\n",
    "    user_id = inputs[\"user_id\"]\n",
    "    current_user_state = \"\"\n",
    "    if current_profile := await get_user_profile(conn, user_id):\n",
    "        current_user_state = _CURRENT_STATE_TEMPLATE.format(\n",
    "            current_user_state=json.dumps(current_profile)\n",
    "        )\n",
    "    converted_messages = []\n",
    "    for m in messages:\n",
    "        if m.type == \"human\":\n",
    "            # Note: this only handles string content\n",
    "            content = f\"<user id={user_id}>{m.content}</user>\"\n",
    "            m = m.__class__(**m.dict(exclude={\"content\"}), content=content)\n",
    "        converted_messages.append(m)\n",
    "    return {\n",
    "        **inputs,\n",
    "        \"current_user_state\": current_user_state,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "\n",
    "async def commit_extraction(pipe_output: dict):\n",
    "    extracted = pipe_output[\"extracted\"]\n",
    "    user_id = pipe_output[\"user_id\"]\n",
    "    await commit_user_profile(conn, user_id, extracted)\n",
    "    return f\"Successfully committed: {extracted.json()} for user {user_id}\"\n",
    "\n",
    "\n",
    "# TODO: Add the retries + persistence. We got some fun tricks up our sleeve for extraction improvements\n",
    "mem_llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n",
    "mem_chain = (\n",
    "    prepare_inputs\n",
    "    | RunnablePassthrough.assign(\n",
    "        extracted=prompt | mem_llm.with_structured_output(UserProfile)\n",
    "    )\n",
    "    | commit_extraction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af12b6-902b-4cd6-b824-db1a806240b9",
   "metadata": {},
   "source": [
    "## Memory Manager\n",
    "\n",
    "It's nice to not have to explicitly trigger memory consolidation after a given thread. In many scenarios, it's impossible to know if a thread has been fully completed!\n",
    "\n",
    "As a balance, we will schedule a consolidation task (aka schedule calls to the `mem_chain` above) whenever a new set of messages are sent to the manager. If an update comes in while that process is still scheduled, we will reset the timer.\n",
    "This reduces redundant calls to the LLM. Feel free to expand on these heuristics (only trigger after convo length has reached size X, only trigger for certain words, run a tiny model or embedding classifier to see if it should trigger, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f777a0-6aaa-436d-9919-3ebc229bcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, mem_chain):\n",
    "        self.mem_chain = mem_chain\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.active_timers = {}\n",
    "\n",
    "    async def enqueue_thread(self, user_id, thread_id, messages, delay=60):\n",
    "        timer_key = (user_id, thread_id)\n",
    "\n",
    "        if timer_key in self.active_timers:\n",
    "            # Cancel the existing timer task\n",
    "            async with self.lock:\n",
    "                if timer_key in self.active_timers:\n",
    "                    (task, _) = self.active_timers[timer_key]\n",
    "                    task.cancel()\n",
    "\n",
    "        async def schedule_ingestion():\n",
    "            await asyncio.sleep(delay)\n",
    "            await self.mem_chain.ainvoke({\"messages\": messages, \"user_id\": user_id})\n",
    "            async with self.lock:\n",
    "                if timer_key in self.active_timers:\n",
    "                    del self.active_timers[timer_key]\n",
    "\n",
    "        # Create a new timer task\n",
    "        task = asyncio.create_task(schedule_ingestion())\n",
    "        async with self.lock:\n",
    "            self.active_timers[timer_key] = (task, messages)\n",
    "\n",
    "    async def trigger(self, user_id=None, thread_id=None):\n",
    "        async def ingest(m, uid, tid):\n",
    "            await self.mem_chain.ainvoke({\"messages\": m, \"user_id\": uid})\n",
    "            async with self.lock:\n",
    "                # not re-entrant so this may be funky\n",
    "                if (uid, tid) in self.active_timers:\n",
    "                    del self.active_timers[(uid, tid)]\n",
    "\n",
    "        if user_id and thread_id:\n",
    "            # Delete and immediately triggger\n",
    "            timer_key = (user_id, thread_id)\n",
    "            if timer_key in self.active_timers:\n",
    "                async with self.lock:\n",
    "                    res = self.active_timers.pop(timer_key, None)\n",
    "                    if res is not None:\n",
    "                        old_task, messages = res\n",
    "                        old_task.cancel()\n",
    "                        task = asyncio.create_task(ingest(messages, user_id, thread_id))\n",
    "                        self.active_timers[timer_key] = (task, messages)\n",
    "        elif user_id is not None:\n",
    "            async with self.lock:\n",
    "                new_tasks = {}\n",
    "                for (uid, tid), (old_task, messages) in self.active_timers.items():\n",
    "                    if uid == user_id:\n",
    "                        task = asyncio.create_task(ingest(messages, user_id, tid))\n",
    "                        new_tasks[(uid, tid)] = (task, messages)\n",
    "                        old_task.cancel()\n",
    "                for k, v in new_tasks.items():\n",
    "                    self.active_timers[k] = v\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59199bd-4a77-4ec9-9aa7-e180d3e1a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = MemoryManager(mem_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817280a8-51c1-42ae-9e52-1d779741c646",
   "metadata": {},
   "source": [
    "## Integrate in your chatbot\n",
    "\n",
    "\n",
    "Define your chatbot below. The key additions are:\n",
    "\n",
    "1. Fetch the user profile from the DB in the entry node. If not present, we don't format it in.\n",
    "2. Schedule memory consolidation after the assistant has responded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c08677-4e0d-4ffa-bfcf-1e8b422b126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI Assistant, equipped with memory about the user (if you have previously interacted with them). Use the core memories below to help shape your conversation.{user_info}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "bot = (\n",
    "    bot_prompt\n",
    "    | ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "    | (lambda x: {\"messages\": x})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932f376-fb9b-49ef-85db-a5ca45e2f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing_extensions import Annotated\n",
    "from typing import TypedDict\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "async def fetch_profile(state: State, config: RunnableConfig):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    profile_str = \"\"\n",
    "    if current_profile := await get_user_profile(conn, user_id):\n",
    "        profile_str = f\"\"\"\n",
    "\n",
    "## User Profile\n",
    "In prior conversations, you have noted the following preferences about the user:\n",
    "<user_profile>\n",
    "{current_profile}\n",
    "</user_profile>\n",
    "Use this as your long term memory of your interactions with the user,\\\n",
    " use it to be a good friend to the user and not forget important information\\\n",
    " about what they've shared. Use it liberally so the user knows you're paying attention.\"\"\"\n",
    "    return {\"user_info\": profile_str}\n",
    "\n",
    "\n",
    "builder.add_node(\"fetch_profile\", fetch_profile)\n",
    "\n",
    "\n",
    "async def process_convo(state: State, config: RunnableConfig):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    delay = config[\"configurable\"].get(\"delay\") or 60\n",
    "    await manager.enqueue_thread(user_id, thread_id, state[\"messages\"], delay=delay)\n",
    "    return {}\n",
    "\n",
    "\n",
    "builder.add_node(\"process_convo\", process_convo)\n",
    "builder.set_entry_point(\"fetch_profile\")\n",
    "builder.add_node(\"bot\", bot)\n",
    "builder.add_edge(\"fetch_profile\", \"bot\")\n",
    "builder.add_edge(\"bot\", \"process_convo\")\n",
    "builder.set_finish_point(\"process_convo\")\n",
    "checkpointer = AsyncSqliteSaver(conn)\n",
    "graph = builder.compile(checkpointer=AsyncSqliteSaver(conn=conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215d54f6-b2ed-47f8-8e07-135d96c8ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb81511-b3b3-4c1b-9dcb-5b2d9f50931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(text: str, user_id: str, thread_id: str):\n",
    "    events = graph.astream(\n",
    "        {\"messages\": [(\"user\", text)]},\n",
    "        {\"configurable\": {\"user_id\": str(user_id), \"thread_id\": str(thread_id)}},\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    async for event in events:\n",
    "        if \"messages\" in event:\n",
    "            messages = event[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if last_message.type == \"ai\":\n",
    "                yield last_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b20c5-1715-48e0-86f5-a92d796b99ad",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "Now let's try interacting with the bot! If you run these quickly, memory will never consolidate, so we will explicitly schedule at the end of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450708b-46ae-4450-81bc-3c1872f550b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"will\"\n",
    "thread_id = \"convo 1\"\n",
    "async for msg in chat(\"Hi there\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4187e-0041-4da1-87f2-69e31403b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for msg in chat(\"Sorry Im testing something!\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169d8a3-1316-4457-982a-ce13894cb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for msg in chat(\"Well it's not that bad i guess!\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35be80-e796-472f-87c1-e0c039be9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for msg in chat(\n",
    "    \"I'm building long term memory for you! then you can know me\", user_id, thread_id\n",
    "):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828f349-7288-460c-b2f1-98c288ab5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for msg in chat(\n",
    "    \"I'm building long term memory for you! then you can know me\", user_id, thread_id\n",
    "):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c1f70-9c39-45b0-9aa8-e1df506b924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await manager.trigger(user_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8f6bd-8cc7-4a78-9e85-970c061b0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thread = \"convo 2\"\n",
    "async for msg in chat(\"Hi there. guess what i'm doing?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c3ad4-a173-483d-8fed-a4764aa2e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"I been working! making some progress\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f296170-51bf-4b7d-aabf-62fdea86d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"Guess what i was working on? Remeber?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9fee9-206b-4d4b-a4b4-82ca281f3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"HI?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
