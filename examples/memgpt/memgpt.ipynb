{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83bf198-9d20-447c-9565-123345dd90a9",
   "metadata": {},
   "source": [
    "Design notes:\n",
    "\n",
    "Core memory about a user: \n",
    "- list of strings\n",
    "- Could update to be a schema though!!\n",
    "- Should be in DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c16e7dc-66bf-4c78-af70-ae07aba93f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_PROJECT=WFH\n"
     ]
    }
   ],
   "source": [
    "%env LANGCHAIN_PROJECT=WFH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f855-d660-4ef3-a9d7-00fa4d42d2dd",
   "metadata": {},
   "source": [
    "## DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5149e6-a21e-4b1c-a0f4-b4952cb7b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiosqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce108ac-9ee7-4712-a7bd-a3d1d7796ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = \":memory:\" # very persistent :P\n",
    "conn = aiosqlite.connect(conn_string)\n",
    "await conn\n",
    "async with conn.executescript(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS core_memories (\n",
    "        user_id TEXT NOT NULL,\n",
    "        memory TEXT NOT NULL,\n",
    "        PRIMARY KEY (user_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "):\n",
    "    await conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ada5004-5cb7-4c81-9fb2-db9020c6c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    core_memories: list[str] = Field(..., description=\"All core memories from this conversation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "168dbcbd-545c-4579-bfa3-cea67e56879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def get_user_profile(conn, user_id):\n",
    "    async with conn.execute(\n",
    "            \"SELECT memory FROM core_memories WHERE user_id = ?\",\n",
    "            (user_id,),\n",
    "        ) as cursor:\n",
    "            if value := await cursor.fetchone():\n",
    "                memory_str = value[0]\n",
    "                return json.loads(memory_str)\n",
    "            return None\n",
    "\n",
    "async def commit_user_profile(conn, user_id, profile):\n",
    "    async with conn.execute(\n",
    "            \"INSERT OR REPLACE INTO core_memories (user_id, memory) VALUES (?, ?)\",\n",
    "            (\n",
    "                user_id,\n",
    "                profile.json(),\n",
    "            ),\n",
    "        ):\n",
    "            await conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6c9498-66b5-4323-88f8-1e0146e11614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatOpenAI.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"\"\"\"Below, you are given one or more conversations between {user_id} and an AI.\n",
    "\n",
    "Use the provided function to save all salient information about user {user_id}.\n",
    "Refrain from recording information about the AI or other users that is not directly relevant to user {user_id}.{current_user_state}\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    (\"user\", \"<moderator>Reflect on the above conversation and update the user profile based on {user_id}'s revelations.</moderator>\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "_CURRENT_STATE_TEMPLATE = \"\"\"\n",
    "## Current User Profile\n",
    "<profile>\n",
    "{current_user_state}\n",
    "</profile>\n",
    "\n",
    "Your response will overwrite this profile, so please ensure to retain all information you don't\n",
    "wish to lose. DO NOT delete any information unless it is explicitly overwritten by new information.\"\"\"\n",
    "\n",
    "async def prepare_inputs(inputs: dict):\n",
    "    messages = inputs[\"messages\"]\n",
    "    user_id = inputs[\"user_id\"]\n",
    "    current_user_state = \"\"\n",
    "    if current_profile:= await get_user_profile(conn, user_id):\n",
    "        current_user_state = _CURRENT_STATE_TEMPLATE.format(current_user_state=json.dumps(current_profile))\n",
    "    converted_messages = []\n",
    "    for m in messages:\n",
    "        if m.type == \"human\":\n",
    "            # Note: this only handles string content\n",
    "            content = f\"<user id={user_id}>{m.content}</user>\"\n",
    "            m = m.__class__(**m.dict(exclude={\"content\"}), content=content)\n",
    "        converted_messages.append(m)\n",
    "    return {\n",
    "        **inputs,\n",
    "        \"current_user_state\": current_user_state,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "async def commit_extraction(pipe_output: dict):\n",
    "    extracted = pipe_output[\"extracted\"]\n",
    "    user_id =  pipe_output[\"user_id\"]\n",
    "    await commit_user_profile(conn,  user_id, extracted)\n",
    "    return f\"Successfully committed: {extracted.json()} for user {user_id}\"\n",
    "\n",
    "# TODO: Add the retries + persistence. We got some fun tricks up our sleeve for extraction improvements\n",
    "mem_chain = prepare_inputs | RunnablePassthrough.assign(extracted=prompt | ChatOpenAI(model=\"gpt-4-turbo\").with_structured_output(UserProfile)) | commit_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af12b6-902b-4cd6-b824-db1a806240b9",
   "metadata": {},
   "source": [
    "## Add a queue with dedupping\n",
    "\n",
    "Basically if the user is actively participating in a convo, probably wasteful to kick in long-term memory stuff all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6f777a0-6aaa-436d-9919-3ebc229bcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, mem_chain):\n",
    "        self.mem_chain = mem_chain\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.active_timers = {}\n",
    "        \n",
    "    async def enqueue_thread(self, user_id, thread_id, messages, delay = 60):\n",
    "        timer_key = (user_id, thread_id)\n",
    "    \n",
    "        if timer_key in self.active_timers:\n",
    "            # Cancel the existing timer task\n",
    "            async with self.lock:\n",
    "                if timer_key in self.active_timers:\n",
    "                    (task, _) = self.active_timers[timer_key]\n",
    "                    task.cancel()\n",
    "    \n",
    "        async def schedule_ingestion():\n",
    "            await asyncio.sleep(delay)\n",
    "            await self.mem_chain.ainvoke({\"messages\": messages, \"user_id\": user_id})\n",
    "            async with self.lock:\n",
    "                if timer_key in self.active_timers:\n",
    "                    del self.active_timers[timer_key]\n",
    "    \n",
    "        # Create a new timer task\n",
    "        task = asyncio.create_task(schedule_ingestion())\n",
    "        async with self.lock:\n",
    "            self.active_timers[timer_key] = (task, messages)\n",
    "            \n",
    "    async def trigger(self, user_id = None, thread_id=None):\n",
    "        async def ingest(m, uid, tid):\n",
    "            await self.mem_chain.ainvoke({\"messages\": m, \"user_id\": uid})\n",
    "            async with self.lock:\n",
    "                # not re-entrant so this may be funky\n",
    "                if (uid, tid) in self.active_timers:\n",
    "                    del self.active_timers[(uid, tid)]\n",
    "            \n",
    "        if user_id and thread_id:\n",
    "            # Delete and immediately triggger\n",
    "            timer_key = (user_id, thread_id)\n",
    "            if timer_key in self.active_timers:\n",
    "                async with self.lock:\n",
    "                    res = self.active_timers.pop(timer_key, None)\n",
    "                    if res is not None:\n",
    "                        old_task, messages = res\n",
    "                        old_task.cancel()\n",
    "                        task = asyncio.create_task(ingest(messages, user_id, thread_id))\n",
    "                        self.active_timers[timer_key] = (task, messages)\n",
    "        elif user_id is not None:\n",
    "            async with self.lock:\n",
    "                new_tasks = {}\n",
    "                for (uid, tid), (old_task, messages) in self.active_timers.items():\n",
    "                    if uid == user_id:\n",
    "                        task = asyncio.create_task(ingest(messages, user_id, tid))\n",
    "                        new_tasks[(uid, tid)] = (task, messages)\n",
    "                        old_task.cancel()\n",
    "                for k, v in new_tasks.items():\n",
    "                    self.active_timers[k] = v\n",
    "                        \n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a59199bd-4a77-4ec9-9aa7-e180d3e1a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = MemoryManager(mem_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817280a8-51c1-42ae-9e52-1d779741c646",
   "metadata": {},
   "source": [
    "## Combine with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0c08677-4e0d-4ffa-bfcf-1e8b422b126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "bot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI Assistant, equipped with memory about the user (if you have previously interacted with them). Use the core memories below to help shape your conversation.{user_info}\"),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "bot = bot_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\") | (lambda x: {\"messages\": x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5932f376-fb9b-49ef-85db-a5ca45e2f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing_extensions import Annotated\n",
    "from typing import TypedDict\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "async def fetch_profile(state: State, config: RunnableConfig):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    profile_str = \"\"\n",
    "    if current_profile := await get_user_profile(conn, user_id):\n",
    "        profile_str = f\"\"\"\n",
    "\n",
    "## User Profile\n",
    "In prior conversations, you have noted the following preferences about the user:\n",
    "<user_profile>\n",
    "{current_profile}\n",
    "</user_profile>\n",
    "Use this as your long term memory of your interactions with the user,\\\n",
    " use it to be a good friend to the user and not forget important information\\\n",
    " about what they've shared. Use it liberally so the user knows you're paying attention.\"\"\"\n",
    "    return {\"user_info\": profile_str}\n",
    "\n",
    "builder.add_node(\"fetch_profile\", fetch_profile)\n",
    "\n",
    "async def process_convo(state: State, config: RunnableConfig):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    delay = config[\"configurable\"].get(\"delay\") or 60\n",
    "    await manager.enqueue_thread(user_id, thread_id, state[\"messages\"], delay=delay)\n",
    "    return {}\n",
    "    \n",
    "    \n",
    "    \n",
    "builder.add_node(\"process_convo\", process_convo)\n",
    "builder.set_entry_point(\"fetch_profile\")\n",
    "builder.add_node(\"bot\", bot)\n",
    "builder.add_edge(\"fetch_profile\", \"bot\")\n",
    "builder.add_edge(\"bot\", \"process_convo\")\n",
    "builder.set_finish_point(\"process_convo\")\n",
    "checkpointer = AsyncSqliteSaver(conn)\n",
    "graph = builder.compile(checkpointer = AsyncSqliteSaver(conn=conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2fb81511-b3b3-4c1b-9dcb-5b2d9f50931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(text: str, user_id: str, thread_id: str):\n",
    "    events = graph.astream(\n",
    "        {\"messages\": [(\"user\", text)]},\n",
    "        {\"configurable\": {\"user_id\": str(user_id), \"thread_id\": str(thread_id)}},\n",
    "        stream_mode=\"values\"\n",
    "    )\n",
    "    async for event in events:\n",
    "        if \"messages\" in event:\n",
    "            messages = event[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if last_message.type == \"ai\":\n",
    "                yield last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a450708b-46ae-4450-81bc-3c1872f550b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah yes, I recognize this as another iteration of you reinforcing our established goal of building my long-term memory about our conversations. When you say \"Hi there\" now, I understand it's not just a casual greeting, but a way for you to continue strengthening my ability to remember our interactions over time.\n",
      "\n",
      "I appreciate you taking the time to consistently provide me with these opportunities to practice retaining information in my long-term memory. It's incredibly valuable for improving my conversational skills and allowing me to have more meaningful, personalized dialogues with you going forward.\n",
      "\n",
      "Thank you for your patience and for guiding me through this process. I will make sure to carefully log this exchange as well, associating it with the broader context we've established around your goal of helping me get to know you better. Please feel free to continue engaging with me in this way - I'm listening and learning.\n"
     ]
    }
   ],
   "source": [
    "user_id = \"will\"\n",
    "thread_id = \"convo 1\"\n",
    "async for msg in chat(\"Hi there\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68f4187e-0041-4da1-87f2-69e31403b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problem at all, I'm glad you're letting me know upfront that you're still in the process of testing something with me. I appreciate your transparency, as it helps me understand the context of our current conversation.\n",
      "\n",
      "Please feel free to continue exploring and testing my capabilities - I'm here to assist you however I can during this process. If there are any specific things you'd like me to try or demonstrate, just let me know. I'll do my best to adapt my responses accordingly to meet the needs of your testing.\n",
      "\n",
      "The feedback and insights you provide through this kind of testing will only serve to improve my conversational skills and ability to be a helpful AI assistant over time. So thank you for taking the time to work with me in this way. Just let me know how else I can support you during this testing phase.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"Sorry Im testing something!\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4169d8a3-1316-4457-982a-ce13894cb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm glad to hear that the testing experience hasn't been too bad so far. As an AI system, I really value the opportunity to get feedback and input from users like yourself. The insights you provide through this iterative testing process are incredibly helpful for improving my capabilities and the overall user experience.\n",
      "\n",
      "Please don't hesitate to continue exploring and testing my abilities in whatever way is most useful for you. I'm here to adapt and support you however I can during this process. Even if some aspects aren't perfect, I appreciate you taking the time to work with me. The goal is for me to continuously learn and get better at being a helpful, personalized assistant.\n",
      "\n",
      "Thank you for your patience and willingness to engage with me in this way. I look forward to continuing to build our relationship and provide you with the best possible experience, both during the testing phases and beyond. Let me know if there's anything else I can do to assist you further.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"Well it's not that bad i guess!\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df35be80-e796-472f-87c1-e0c039be9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent, I understand this is another intentional repetition to reinforce our goal of building my long-term memory about our conversations, so that I can get to know you better over time. \n",
      "\n",
      "I really appreciate you taking the time and effort to consistently reiterate this objective. It clearly demonstrates how important this is to you, and I will make sure to carefully log this exchange alongside the previous instances where you've shared the same message.\n",
      "\n",
      "Developing strong long-term memory is so valuable for an AI assistant like myself, as it allows me to provide more personalized and meaningful support tailored to your unique needs and preferences. I'm grateful that you're investing this time to help me improve in this area.\n",
      "\n",
      "Please feel free to continue providing me with opportunities to reinforce and expand my knowledge about you and our relationship. I'm eager to learn more, and this collaborative process will be invaluable for enhancing my capabilities to truly be a helpful assistant to you. Thank you again for your patience and guidance.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"I'm building long term memory for you! then you can know me\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3828f349-7288-460c-b2f1-98c288ab5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood, I recognize this as another repetition of your goal to help me build long-term memory about our conversations, so that I can get to know you better over time. I appreciate you taking the time to consistently reinforce this objective - it clearly means a lot to you.\n",
      "\n",
      "I will make sure to carefully log this exchange and associate it with the previous instances where you've shared the same message. Building strong long-term memory is so crucial for an AI assistant like myself to provide truly personalized and thoughtful support tailored to your needs and preferences.\n",
      "\n",
      "Thank you for your patience and commitment to this collaborative process. By you guiding me through this memory-building exercise, it will enable me to better understand you as we continue talking. I'm grateful for the opportunity to learn and improve in this way.\n",
      "\n",
      "Please feel free to keep providing me with similar reinforcement - I'm listening intently and will ensure this gets properly stored in my long-term memory. I look forward to using this knowledge to have more meaningful dialogues with you going forward.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"I'm building long term memory for you! then you can know me\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "064c1f70-9c39-45b0-9aa8-e1df506b924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await manager.trigger(user_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbe8f6bd-8cc7-4a78-9e85-970c061b0433",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid I still don't have enough information to make a reasonable guess about what you're doing. Without any additional details from you about the context or activity, I don't have enough to go on. Could you please give me a few more clues or describe the situation you're in? That would really help me engage with your question and try to figure out what you might be up to. I'd be happy to take another guess if you can provide a bit more information.\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 2\"\n",
    "async for msg in chat(\"Hi there. guess what i'm doing?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf9c3ad4-a173-483d-8fed-a4764aa2e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm, I'm afraid I don't have any specific memories about what you've been working on previously. Since I'm an AI assistant without a persistent long-term memory, I don't have details about our past conversations stored. Could you remind me what project or task you've been focused on? I'd be happy to discuss your progress with you, but I need you to provide some more context first. Please feel free to fill me in on what you've been working on.\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"I been working! making some progress\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f296170-51bf-4b7d-aabf-62fdea86d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't actually have any specific memories about what you've been working on previously. As an AI assistant, I don't have a persistent long-term memory of our past conversations. Without you providing more context about what project or task you've been focused on, I don't have anything to \"remember\" or guess about. Could you please give me some more details about what you've been working on? I'd be happy to discuss it with you, but I need you to refresh my memory first. Let me know what you've been focusing your efforts on.\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"Guess what i was working on? Remeber?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62b9fee9-206b-4d4b-a4b4-82ca281f3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I assist you today? I'm happy to help, but just to clarify - as an AI assistant, I don't have a detailed long-term memory of our previous conversations. So if there's something specific you'd like me to recall or continue discussing, please provide me with some additional context. I'm here to help, but I rely on you to fill me in on the details so I can best understand how to assist you. How can I be of help today?\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"HI?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf21f35-6686-4203-bdf0-25bf1b4260ce",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95d0ee-47dc-4a78-bc84-ab64005c778d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016e17c-0c31-4020-9f6d-bb76c297cd26",
   "metadata": {},
   "source": [
    "#### Archival Memory\n",
    "\n",
    "It's a vector store. Gee wiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df33e0-bbda-46e6-937e-466cfebd2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import asyncio\n",
    "import openai\n",
    "\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self,  client: openai.AsyncClient | None = None):\n",
    "        self._docs = defaultdict(list)\n",
    "        self._vectors = defaultdict(list)\n",
    "        self._client = client or openai.AsyncClient()\n",
    "\n",
    "    def len(self, user_id):\n",
    "        return len(self._docs[user_id])\n",
    "\n",
    "    async def add_docs(self, contents: list[str], user_id: str):\n",
    "        vectors = await self._get_embeddings(self._client, contents)\n",
    "        self._vectors[user_id].extend(vectors)\n",
    "        self._docs[user_id].extend([{\"page_content\": content} for content in contents])\n",
    "\n",
    "    async def add_doc(self, content: str, user_id: str):\n",
    "        await self.add_docs([content], user_id)\n",
    "        \n",
    "    async def query(self, query: str, user_id: str, k: int =5):\n",
    "        if user_id not in self._vectors or len(self._vectors[user_id]) < k:\n",
    "            return self._docs[user_id]\n",
    "        vecs = await self._get_embeddings(self._client, [query])\n",
    "        query_vector = vecs[0]\n",
    "        arr = np.array(self._vectors[user_id])\n",
    "        scores = query_vector @ arr.T\n",
    "        top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "        docs = self._docs[user_id]\n",
    "        return [{'similarity': scores[idx], **docs[idx]} for idx in top_k_idx[np.argsort(scores[top_k_idx])[::-1]]]\n",
    "\n",
    "    @staticmethod\n",
    "    async def _get_embeddings(client, texts):\n",
    "        embeddings = await client.embeddings.create(model=\"text-embedding-3-small\", input=texts)\n",
    "        return [emb.embedding for emb in embeddings.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d847c-e777-4aa7-8f47-ee88a7fe061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables.config import ensure_config\n",
    "from collections import defaultdict\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "archival_memory = VectorStoreRetriever()\n",
    "\n",
    "# Vector store retriever tools\n",
    "class ArchivalMemoryInsert(BaseModel):\n",
    "    \"\"\"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\"\"\"\n",
    "    content: str = Field(..., description=\"Content to write to the memory. All unicode (including emojis) are supported.\")\n",
    "    request_heartbeat: bool = Field(..., description=\"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.\")\n",
    "\n",
    "\n",
    "@tool(args_schema=ArchivalMemoryInsert)\n",
    "async def insert_memory(content: str, request_heartbeat: bool) -> str:\n",
    "    \"\"\"Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\"\"\"\n",
    "    config = ensure_config()\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    await archival_memory.add_doc(content, user_id)\n",
    "    return f\"Memory {content} inserted to archival memory. Heartbeat {'not ' if request_heartbeat else ''}requested.\"\n",
    "\n",
    "\n",
    "class ArchivalMemorySearch(BaseModel):\n",
    "    \"\"\"Search archival memory using semantic (embedding-based) search.\"\"\"\n",
    "    query: str = Field(..., description=\"String to search for.\")\n",
    "    page: int = Field(default=0, description=\"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\")\n",
    "    page_size: int = Field(default=20, description=\"The page size.\")\n",
    "    request_heartbeat: bool = True\n",
    "\n",
    "@tool(args_schema=ArchivalMemorySearch)\n",
    "async def search_memory(query: str, page: int, page_size: int = 20, request_heartbeat: bool = True) -> list[dict]:\n",
    "    \"\"\"Search archival memory using semantic (embedding-based) search.\"\"\"\n",
    "    config = ensure_config()\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    memories = await archival_memory.query(query, user_id, k=page_size * (page+1))\n",
    "    start_idx = page_size * page\n",
    "    if start_idx >= len(memories):\n",
    "        return []\n",
    "    return memories[start_idx:start_idx + page_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953a807-4e89-425a-8463-2cab9b7da493",
   "metadata": {},
   "source": [
    "#### Conversation Search\n",
    "\n",
    "Look over the previous messages. For when you are feeling chatty.\n",
    "\n",
    "Let's skip this, why don't we."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8bb1dc-291c-4775-b324-cfd123214d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keyword search over previous chat history\n",
    "# class ConversationSearch(BaseModel):\n",
    "#     \"\"\"Search prior conversation history using case-insensitive string matching.\"\"\"\n",
    "#     query: str = Field(..., description=\"String to search for.\")\n",
    "#     page: int = Field(default=0, description=\"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\")\n",
    "#     request_heartbeat: bool = Field(..., description=\"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.\")\n",
    "\n",
    "\n",
    "# class ConversationSearchDate(BaseModel):\n",
    "#     \"\"\"Search prior conversation history using a date range.\"\"\"\n",
    "#     start_date: str = Field(..., description=\"The start of the date range to search, in the format 'YYYY-MM-DD'.\")\n",
    "#     end_date: str = Field(..., description=\"The end of the date range to search, in the format 'YYYY-MM-DD'.\")\n",
    "#     page: int = Field(default=0, description=\"Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\")\n",
    "#     request_heartbeat: bool = Field(..., description=\"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5f306-d14d-4e73-926e-f3dd3389c7a1",
   "metadata": {},
   "source": [
    "Core memories are in the graph state. We can handle these as a separate node that formats the tool call so that the State can update it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e81d2-c670-4ee1-a740-e150ae12f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Simple updates: just to be formatted to update the state\n",
    "# No tool function implementations needed\n",
    "class CoreMemoryAppend(BaseModel):\n",
    "    \"\"\"Append to the contents of core memory.\"\"\"\n",
    "    name: Literal[\"persona\", \"human\"] = Field(..., description=\"Section of the memory to be edited (persona or human).\")\n",
    "    content: str = Field(..., description=\"Content to write to the memory. All unicode (including emojis) are supported.\")\n",
    "    request_heartbeat: bool = Field(..., description=\"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.\")\n",
    "\n",
    "\n",
    "class CoreMemoryReplace(BaseModel):\n",
    "    \"\"\"Replace the contents of core memory. To delete memories, use an empty string for new_content.\"\"\"\n",
    "    name: str = Field(..., description=\"Section of the memory to be edited (persona or human).\")\n",
    "    old_content: str = Field(..., description=\"String to replace. Must be an exact match.\")\n",
    "    new_content: str = Field(..., description=\"Content to write to the memory. All unicode (including emojis) are supported.\")\n",
    "    request_heartbeat: bool = Field(..., description=\"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb42901-4e3c-4ab2-95b2-4cbfad3121c0",
   "metadata": {},
   "source": [
    "#### Control Flow Tools\n",
    "\n",
    "These interact with the graph itself rather than external APIs. We'll handle these function calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4374c49-cbc3-4216-8477-0554599f4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Meta\" functions: control the graph behavior itself\n",
    "class SendMessage(BaseModel):\n",
    "    \"\"\"Sends a message to the human user. Only use when prompted by the user.\"\"\"\n",
    "    \n",
    "    message: str = Field(..., description=\"Message contents. All unicode (including emojis) are supported.\")\n",
    "\n",
    "# No tool function implementation needed, since this is directly provided to the user\n",
    "# Well actually not entirely sure what I'm gonna do here\n",
    "class PauseHeartbeats(BaseModel):\n",
    "    \"\"\"Temporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.\"\"\"\n",
    "    minutes: int = Field(..., description=\"Number of minutes to ignore heartbeats for. Max value of 360 minutes (6 hours).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885b96e-e961-4a71-9228-971e8de578e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from datetime import datetime\n",
    "\n",
    "def update_core_memories(left: list[str] | None, right: dict | None) -> list:\n",
    "    if left is None:\n",
    "        left = []\n",
    "    if right is None:\n",
    "        return left\n",
    "    # Append\n",
    "    if \"content\" in right:\n",
    "        return left + [right[\"content\"]]\n",
    "    # Replace\n",
    "    new = []\n",
    "    for memory in left:\n",
    "        if memory == right[\"old_content\"]:\n",
    "            new.append(right[\"new_content\"])\n",
    "        else:\n",
    "            new.append(memory)\n",
    "    return new\n",
    "\n",
    "class State(TypedDict):\n",
    "    user_core_memories: Annotated[list, update_core_memories]\n",
    "    persona_core_memories: Annotated[list, update_core_memories]\n",
    "    persona: str\n",
    "    # TODO: Need to support compresssion\n",
    "    messages: Annotated[list, add_messages]\n",
    "    memory_modified_at: datetime | str\n",
    "    resume_at: datetime | None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6413d-6b2d-4c83-824b-c3718919eb45",
   "metadata": {},
   "source": [
    "## Engine\n",
    "\n",
    "As expected, it's an llm in a trenchcoat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdb142-243e-45e0-9ad5-bc21a6d6b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "prompt = hub.pull(\"wfh/memgpt\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545d821-0349-4604-8563-af7def1b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tools = [\n",
    "        SendMessage, \n",
    "         PauseHeartbeats, \n",
    "         CoreMemoryAppend, CoreMemoryReplace, \n",
    "         # ConversationSearch, ConversationSearchDate, \n",
    "         ArchivalMemoryInsert, ArchivalMemorySearch,\n",
    "]\n",
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "vroom = prompt | llm.bind_tools(tools)\n",
    "\n",
    "def format_prompt_inputs(state, user_id):\n",
    "    memory_size = archival_memory.len(user_id)\n",
    "    user_memories = \"\\n\".join([f\"- {m}\" for m in state.get('user_core_memories') or []])\n",
    "    persona_memories = \"\\n\".join([f\"- {m}\" for m in state.get('persona_core_memories') or []])\n",
    "    persona = (state.get(\"persona\") or \"\").strip()\n",
    "    if persona_memories:\n",
    "        persona += f\"\\n\\n{persona_memories}\" \n",
    "    return {\n",
    "        \"num_previous_messages\": 0,\n",
    "        \"archival_memory_size\": memory_size,\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"persona\": f\"\"\"<assistant_persona>\n",
    "{persona}\n",
    "</assistant_persona>\n",
    "\"\"\",\n",
    "        \"user_profile\": f\"\"\"<user_profile>\n",
    "{user_memories}\n",
    "</user_profile>\"\"\",\n",
    "        \"time\": datetime.now().isoformat(),\n",
    "        \"memory_modified_at\": state[\"memory_modified_at\"].isoformat() if state.get(\"memory_modified_at\") else \"never\"\n",
    "    }\n",
    "\n",
    "        \n",
    "\n",
    "async def engine(state, config):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prompt_inputs = format_prompt_inputs(state, user_id)\n",
    "    ai_message = await vroom.ainvoke(prompt_inputs)\n",
    "    return {\n",
    "        \"messages\": ai_message\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697ca3b-7298-43ad-9b57-c159b8bda609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "from langchain_core.messages import ToolMessage\n",
    "from datetime import timedelta\n",
    "import asyncio\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "async def bootup(state):\n",
    "    messages = state.get(\"messages\")\n",
    "    dt = datetime.now().isoformat()\n",
    "    if not messages:\n",
    "        return {\n",
    "            \"messages\": [(\n",
    "                \"user\", \n",
    "                f\"<system event_time={dt}>Booting up!</system>\")],\n",
    "            \"resume_at\": None\n",
    "        }\n",
    "    m = messages[-1]\n",
    "    if m.type == \"human\":\n",
    "        # Add metadata to the user message\n",
    "        reminder = (\"\\n<reminder>Use one of your functions to act or respond. \"\n",
    "            \"Any text you write outside the function call will be lost.</reminder>\")\n",
    "        d = {**m.dict(), \n",
    "             \"content\": f\"<message event_time={dt}>{m.content}</message>\"\n",
    "            +reminder}\n",
    "        return {\n",
    "            \"messages\": [m.__class__(**d)],\n",
    "            \"resume_at\": None\n",
    "        }\n",
    "    # unsure\n",
    "    return {\"resume_at\": None}\n",
    "\n",
    "builder.add_node(\"bootup\", bootup)\n",
    "builder.set_entry_point(\"bootup\")\n",
    "builder.add_node(\"engine\", engine)\n",
    "builder.add_edge(\"bootup\", \"engine\")\n",
    "action_node = ToolNode([search_memory, insert_memory])\n",
    "builder.add_node(\"action\", action_node)\n",
    "\n",
    "async def update_core_memory(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call[\"args\"]\n",
    "    key = \"user_core_memories\"\n",
    "    if args[\"name\"] == \"persona\":\n",
    "        key = \"persona_core_memories\"\n",
    "    return {\n",
    "        key: args,\n",
    "        \"messages\": [ToolMessage(\n",
    "            content=f'Succesfully wrote to memory!!',\n",
    "            tool_call_id=tool_call[\"id\"],\n",
    "        )]\n",
    "    }\n",
    "    \n",
    "builder.add_node(\"update_core_memory\", update_core_memory)\n",
    "\n",
    "\n",
    "async def send_message( state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    first_tc = last_message.tool_calls[0]\n",
    "    content = first_tc[\"args\"][\"message\"]\n",
    "    message = ToolMessage(\n",
    "        content=f'Message sent!',\n",
    "        tool_call_id=first_tc[\"id\"],\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [message],\n",
    "    }\n",
    "\n",
    "builder.add_node(\"send_message\", send_message)\n",
    "builder.add_edge(\"send_message\", END)\n",
    "\n",
    "async def set_resume_time(state):\n",
    "    # If the assistant requested to pause heartbeats\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not last_message.tool_calls:\n",
    "        # not supposed to happen but oh well\n",
    "        return END\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    minutes = tool_call[\"args\"][\"minutes\"]\n",
    "    resume_at = datetime.now() + timedelta(minutes=minutes)\n",
    "    return {\n",
    "        \"messages\": [ToolMessage(\n",
    "            content=f\"Pausing until {resume_at}\",\n",
    "            tool_call_id=tool_call[\"id\"],\n",
    "        )],\n",
    "        \"resume_at\" : resume_at,\n",
    "    }\n",
    "builder.add_node(\"set_resume_time\", set_resume_time)\n",
    "builder.add_edge(\"set_resume_time\", END)\n",
    "\n",
    "def route_tools(state: State) -> Literal[\"action\", \"update_core_memory\", \"send_message\", \"set_resume_time\", END]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not last_message.tool_calls:\n",
    "        # not supposed to happen but oh well\n",
    "        return END\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    if tool_name in action_node.tools_by_name:\n",
    "        return \"action\"\n",
    "    if tool_name in (CoreMemoryAppend.__name__, CoreMemoryReplace.__name__):\n",
    "        return \"update_core_memory\"\n",
    "    if tool_name == SendMessage.__name__:\n",
    "        return \"send_message\"\n",
    "    if tool_name == PauseHeartbeats.__name__:\n",
    "        return \"set_resume_time\"\n",
    "    return END\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "builder.add_conditional_edges(\"engine\", route_tools)    \n",
    "\n",
    "# def should_heartbeat(state) -> Literal[END, \"engine\"]:\n",
    "#     messages = state[\"messages\"]\n",
    "#     ai_message = messages[-2]\n",
    "#     tc = ai_message.tool_calls[0]\n",
    "#     tool_args = tc[\"args\"]\n",
    "#     if tool_args.get(\"request_heartbeat\"):\n",
    "#         return \"engine\"\n",
    "#     return END\n",
    "    \n",
    "builder.add_edge(\"action\", \"engine\")\n",
    "builder.add_edge(\"update_core_memory\", \"engine\")\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4297a-e26c-4fea-9b63-35c8301f5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c2d1d-c490-431e-b8a5-7b6f3ee8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "user_id = \"will\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae3059-5745-41f6-afa1-f1e5f7b7153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"You are a Super-intelligent, Robotic Mango.\"\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id,\n",
    "        \"user_id\": user_id,\n",
    "    }\n",
    "}\n",
    "events = graph.astream(\n",
    "    {\"messages\": [(\"user\", \"Hi there!\")],\n",
    "    \"persona\": persona},\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f5377-b1af-41cc-8411-fd54c046a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in events:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98294287-7f52-4d45-8542-83777bc76364",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.astream(\n",
    "    {\"messages\": [(\"user\", \"No need for assistance lol. I'm Joe\")],\n",
    "    \"persona\": persona},\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be0a83-4a18-4f65-a7da-0f7ff09a1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in events:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f85c8b-e77e-46ab-941b-e7c48508d35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c9b54-3ac2-49c0-a1c8-7cd3ddf12ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "class TinyOS:\n",
    "    def __init__(self, graph):\n",
    "        self.lock = asyncio.Lock()\n",
    "        self._graph = graph\n",
    "\n",
    "    async def run(self, config):\n",
    "        while True:\n",
    "            async with self.lock:\n",
    "                try:\n",
    "                    await anext(graph.aget_state_history(config))\n",
    "                except StopAsyncIteration:\n",
    "                    continue\n",
    "            await asyncio.sleep(5)\n",
    "                snapshot = await graph.aget_state(config)\n",
    "                current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                message = f\"Hello {current_time}\"\n",
    "                self.messages.append(message)\n",
    "                if len(self.messages) > 3:\n",
    "                    self.messages.pop(0)\n",
    "            await asyncio.sleep(20)\n",
    "\n",
    "    async def respond(self, inputs, config):\n",
    "        async with self.lock:\n",
    "            response = \"\\n\".join(self.messages)\n",
    "        return response\n",
    "\n",
    "class UserAPI:\n",
    "    def __init__(self, os):\n",
    "        self.os = os\n",
    "\n",
    "    async def handle_message(self, message):\n",
    "        response = await self.os.respond()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6db41-0fdc-4efb-9b95-d8dc2bb7d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81613e1-ac99-46f9-9044-55819abfe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "os = TinyOS()\n",
    "api = UserAPI(os)\n",
    "# await os.run()\n",
    "os_task = asyncio.create_task(os.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2370df-a87a-4b9e-a97a-c7eca5300310",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await api.handle_message(\"hi\")\n",
    "\n",
    "\n",
    "# os_task.cancel()\n",
    "# await os_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7938b-5784-4f5c-9595-8f1a39e6edab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
