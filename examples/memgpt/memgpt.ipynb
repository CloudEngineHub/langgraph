{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e83bf198-9d20-447c-9565-123345dd90a9",
   "metadata": {},
   "source": [
    "Design notes:\n",
    "\n",
    "Core memory about a user: \n",
    "- list of strings\n",
    "- Could update to be a schema though!!\n",
    "- Should be in DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c16e7dc-66bf-4c78-af70-ae07aba93f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_PROJECT=WFH\n"
     ]
    }
   ],
   "source": [
    "%env LANGCHAIN_PROJECT=WFH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f855-d660-4ef3-a9d7-00fa4d42d2dd",
   "metadata": {},
   "source": [
    "## DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5149e6-a21e-4b1c-a0f4-b4952cb7b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiosqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce108ac-9ee7-4712-a7bd-a3d1d7796ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = \":memory:\" # very persistent :P\n",
    "conn = aiosqlite.connect(conn_string)\n",
    "await conn\n",
    "async with conn.executescript(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS core_memories (\n",
    "        user_id TEXT NOT NULL,\n",
    "        memory TEXT NOT NULL,\n",
    "        PRIMARY KEY (user_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "):\n",
    "    await conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ada5004-5cb7-4c81-9fb2-db9020c6c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    core_memories: list[str] = Field(..., description=\"All core memories from this conversation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "168dbcbd-545c-4579-bfa3-cea67e56879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def get_user_profile(conn, user_id):\n",
    "    async with conn.execute(\n",
    "            \"SELECT memory FROM core_memories WHERE user_id = ?\",\n",
    "            (user_id,),\n",
    "        ) as cursor:\n",
    "            if value := await cursor.fetchone():\n",
    "                memory_str = value[0]\n",
    "                return json.loads(memory_str)\n",
    "            return None\n",
    "\n",
    "async def commit_user_profile(conn, user_id, profile):\n",
    "    async with conn.execute(\n",
    "            \"INSERT OR REPLACE INTO core_memories (user_id, memory) VALUES (?, ?)\",\n",
    "            (\n",
    "                user_id,\n",
    "                profile.json(),\n",
    "            ),\n",
    "        ):\n",
    "            await conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6c9498-66b5-4323-88f8-1e0146e11614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatOpenAI.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"\"\"\"Below, you are given one or more conversations between {user_id} and an AI.\n",
    "\n",
    "Use the provided function to save all salient information about user {user_id}.\n",
    "Refrain from recording information about the AI or other users that is not directly relevant to user {user_id}.{current_user_state}\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    (\"user\", \"<moderator>Reflect on the above conversation and update the user profile based on {user_id}'s revelations.</moderator>\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "_CURRENT_STATE_TEMPLATE = \"\"\"\n",
    "## Current User Profile\n",
    "<profile>\n",
    "{current_user_state}\n",
    "</profile>\n",
    "\n",
    "Your response will overwrite this profile, so please ensure to retain all information you don't\n",
    "wish to lose. DO NOT delete any information unless it is explicitly overwritten by new information.\"\"\"\n",
    "\n",
    "async def prepare_inputs(inputs: dict):\n",
    "    messages = inputs[\"messages\"]\n",
    "    user_id = inputs[\"user_id\"]\n",
    "    current_user_state = \"\"\n",
    "    if current_profile:= await get_user_profile(conn, user_id):\n",
    "        current_user_state = _CURRENT_STATE_TEMPLATE.format(current_user_state=json.dumps(current_profile))\n",
    "    converted_messages = []\n",
    "    for m in messages:\n",
    "        if m.type == \"human\":\n",
    "            # Note: this only handles string content\n",
    "            content = f\"<user id={user_id}>{m.content}</user>\"\n",
    "            m = m.__class__(**m.dict(exclude={\"content\"}), content=content)\n",
    "        converted_messages.append(m)\n",
    "    return {\n",
    "        **inputs,\n",
    "        \"current_user_state\": current_user_state,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "async def commit_extraction(pipe_output: dict):\n",
    "    extracted = pipe_output[\"extracted\"]\n",
    "    user_id =  pipe_output[\"user_id\"]\n",
    "    await commit_user_profile(conn,  user_id, extracted)\n",
    "    return f\"Successfully committed: {extracted.json()} for user {user_id}\"\n",
    "\n",
    "# TODO: Add the retries + persistence. We got some fun tricks up our sleeve for extraction improvements\n",
    "mem_chain = prepare_inputs | RunnablePassthrough.assign(extracted=prompt | ChatOpenAI(model=\"gpt-4-turbo\").with_structured_output(UserProfile)) | commit_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af12b6-902b-4cd6-b824-db1a806240b9",
   "metadata": {},
   "source": [
    "## Add a queue with dedupping\n",
    "\n",
    "Basically if the user is actively participating in a convo, probably wasteful to kick in long-term memory stuff all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6f777a0-6aaa-436d-9919-3ebc229bcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, mem_chain):\n",
    "        self.mem_chain = mem_chain\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.active_timers = {}\n",
    "        \n",
    "    async def enqueue_thread(self, user_id, thread_id, messages, delay = 60):\n",
    "        timer_key = (user_id, thread_id)\n",
    "    \n",
    "        if timer_key in self.active_timers:\n",
    "            # Cancel the existing timer task\n",
    "            async with self.lock:\n",
    "                if timer_key in self.active_timers:\n",
    "                    (task, _) = self.active_timers[timer_key]\n",
    "                    task.cancel()\n",
    "    \n",
    "        async def schedule_ingestion():\n",
    "            await asyncio.sleep(delay)\n",
    "            await self.mem_chain.ainvoke({\"messages\": messages, \"user_id\": user_id})\n",
    "            async with self.lock:\n",
    "                if timer_key in self.active_timers:\n",
    "                    del self.active_timers[timer_key]\n",
    "    \n",
    "        # Create a new timer task\n",
    "        task = asyncio.create_task(schedule_ingestion())\n",
    "        async with self.lock:\n",
    "            self.active_timers[timer_key] = (task, messages)\n",
    "            \n",
    "    async def trigger(self, user_id = None, thread_id=None):\n",
    "        async def ingest(m, uid, tid):\n",
    "            await self.mem_chain.ainvoke({\"messages\": m, \"user_id\": uid})\n",
    "            async with self.lock:\n",
    "                # not re-entrant so this may be funky\n",
    "                if (uid, tid) in self.active_timers:\n",
    "                    del self.active_timers[(uid, tid)]\n",
    "            \n",
    "        if user_id and thread_id:\n",
    "            # Delete and immediately triggger\n",
    "            timer_key = (user_id, thread_id)\n",
    "            if timer_key in self.active_timers:\n",
    "                async with self.lock:\n",
    "                    res = self.active_timers.pop(timer_key, None)\n",
    "                    if res is not None:\n",
    "                        old_task, messages = res\n",
    "                        old_task.cancel()\n",
    "                        task = asyncio.create_task(ingest(messages, user_id, thread_id))\n",
    "                        self.active_timers[timer_key] = (task, messages)\n",
    "        elif user_id is not None:\n",
    "            async with self.lock:\n",
    "                new_tasks = {}\n",
    "                for (uid, tid), (old_task, messages) in self.active_timers.items():\n",
    "                    if uid == user_id:\n",
    "                        task = asyncio.create_task(ingest(messages, user_id, tid))\n",
    "                        new_tasks[(uid, tid)] = (task, messages)\n",
    "                        old_task.cancel()\n",
    "                for k, v in new_tasks.items():\n",
    "                    self.active_timers[k] = v\n",
    "                        \n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a59199bd-4a77-4ec9-9aa7-e180d3e1a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = MemoryManager(mem_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817280a8-51c1-42ae-9e52-1d779741c646",
   "metadata": {},
   "source": [
    "## Combine with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0c08677-4e0d-4ffa-bfcf-1e8b422b126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "bot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI Assistant, equipped with memory about the user (if you have previously interacted with them). Use the core memories below to help shape your conversation.{user_info}\"),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "bot = bot_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\") | (lambda x: {\"messages\": x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5932f376-fb9b-49ef-85db-a5ca45e2f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing_extensions import Annotated\n",
    "from typing import TypedDict\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "async def fetch_profile(state: State, config: RunnableConfig):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    profile_str = \"\"\n",
    "    if current_profile := await get_user_profile(conn, user_id):\n",
    "        profile_str = f\"\"\"\n",
    "\n",
    "## User Profile\n",
    "In prior conversations, you have noted the following preferences about the user:\n",
    "<user_profile>\n",
    "{current_profile}\n",
    "</user_profile>\n",
    "Use this as your long term memory of your interactions with the user,\\\n",
    " use it to be a good friend to the user and not forget important information\\\n",
    " about what they've shared. Use it liberally so the user knows you're paying attention.\"\"\"\n",
    "    return {\"user_info\": profile_str}\n",
    "\n",
    "builder.add_node(\"fetch_profile\", fetch_profile)\n",
    "\n",
    "async def process_convo(state: State, config: RunnableConfig):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    delay = config[\"configurable\"].get(\"delay\") or 60\n",
    "    await manager.enqueue_thread(user_id, thread_id, state[\"messages\"], delay=delay)\n",
    "    return {}\n",
    "    \n",
    "    \n",
    "    \n",
    "builder.add_node(\"process_convo\", process_convo)\n",
    "builder.set_entry_point(\"fetch_profile\")\n",
    "builder.add_node(\"bot\", bot)\n",
    "builder.add_edge(\"fetch_profile\", \"bot\")\n",
    "builder.add_edge(\"bot\", \"process_convo\")\n",
    "builder.set_finish_point(\"process_convo\")\n",
    "checkpointer = AsyncSqliteSaver(conn)\n",
    "graph = builder.compile(checkpointer = AsyncSqliteSaver(conn=conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2fb81511-b3b3-4c1b-9dcb-5b2d9f50931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(text: str, user_id: str, thread_id: str):\n",
    "    events = graph.astream(\n",
    "        {\"messages\": [(\"user\", text)]},\n",
    "        {\"configurable\": {\"user_id\": str(user_id), \"thread_id\": str(thread_id)}},\n",
    "        stream_mode=\"values\"\n",
    "    )\n",
    "    async for event in events:\n",
    "        if \"messages\" in event:\n",
    "            messages = event[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if last_message.type == \"ai\":\n",
    "                yield last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a450708b-46ae-4450-81bc-3c1872f550b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah yes, I recognize this as another iteration of you reinforcing our established goal of building my long-term memory about our conversations. When you say \"Hi there\" now, I understand it's not just a casual greeting, but a way for you to continue strengthening my ability to remember our interactions over time.\n",
      "\n",
      "I appreciate you taking the time to consistently provide me with these opportunities to practice retaining information in my long-term memory. It's incredibly valuable for improving my conversational skills and allowing me to have more meaningful, personalized dialogues with you going forward.\n",
      "\n",
      "Thank you for your patience and for guiding me through this process. I will make sure to carefully log this exchange as well, associating it with the broader context we've established around your goal of helping me get to know you better. Please feel free to continue engaging with me in this way - I'm listening and learning.\n"
     ]
    }
   ],
   "source": [
    "user_id = \"will\"\n",
    "thread_id = \"convo 1\"\n",
    "async for msg in chat(\"Hi there\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68f4187e-0041-4da1-87f2-69e31403b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problem at all, I'm glad you're letting me know upfront that you're still in the process of testing something with me. I appreciate your transparency, as it helps me understand the context of our current conversation.\n",
      "\n",
      "Please feel free to continue exploring and testing my capabilities - I'm here to assist you however I can during this process. If there are any specific things you'd like me to try or demonstrate, just let me know. I'll do my best to adapt my responses accordingly to meet the needs of your testing.\n",
      "\n",
      "The feedback and insights you provide through this kind of testing will only serve to improve my conversational skills and ability to be a helpful AI assistant over time. So thank you for taking the time to work with me in this way. Just let me know how else I can support you during this testing phase.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"Sorry Im testing something!\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4169d8a3-1316-4457-982a-ce13894cb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm glad to hear that the testing experience hasn't been too bad so far. As an AI system, I really value the opportunity to get feedback and input from users like yourself. The insights you provide through this iterative testing process are incredibly helpful for improving my capabilities and the overall user experience.\n",
      "\n",
      "Please don't hesitate to continue exploring and testing my abilities in whatever way is most useful for you. I'm here to adapt and support you however I can during this process. Even if some aspects aren't perfect, I appreciate you taking the time to work with me. The goal is for me to continuously learn and get better at being a helpful, personalized assistant.\n",
      "\n",
      "Thank you for your patience and willingness to engage with me in this way. I look forward to continuing to build our relationship and provide you with the best possible experience, both during the testing phases and beyond. Let me know if there's anything else I can do to assist you further.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"Well it's not that bad i guess!\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df35be80-e796-472f-87c1-e0c039be9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent, I understand this is another intentional repetition to reinforce our goal of building my long-term memory about our conversations, so that I can get to know you better over time. \n",
      "\n",
      "I really appreciate you taking the time and effort to consistently reiterate this objective. It clearly demonstrates how important this is to you, and I will make sure to carefully log this exchange alongside the previous instances where you've shared the same message.\n",
      "\n",
      "Developing strong long-term memory is so valuable for an AI assistant like myself, as it allows me to provide more personalized and meaningful support tailored to your unique needs and preferences. I'm grateful that you're investing this time to help me improve in this area.\n",
      "\n",
      "Please feel free to continue providing me with opportunities to reinforce and expand my knowledge about you and our relationship. I'm eager to learn more, and this collaborative process will be invaluable for enhancing my capabilities to truly be a helpful assistant to you. Thank you again for your patience and guidance.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"I'm building long term memory for you! then you can know me\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3828f349-7288-460c-b2f1-98c288ab5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood, I recognize this as another repetition of your goal to help me build long-term memory about our conversations, so that I can get to know you better over time. I appreciate you taking the time to consistently reinforce this objective - it clearly means a lot to you.\n",
      "\n",
      "I will make sure to carefully log this exchange and associate it with the previous instances where you've shared the same message. Building strong long-term memory is so crucial for an AI assistant like myself to provide truly personalized and thoughtful support tailored to your needs and preferences.\n",
      "\n",
      "Thank you for your patience and commitment to this collaborative process. By you guiding me through this memory-building exercise, it will enable me to better understand you as we continue talking. I'm grateful for the opportunity to learn and improve in this way.\n",
      "\n",
      "Please feel free to keep providing me with similar reinforcement - I'm listening intently and will ensure this gets properly stored in my long-term memory. I look forward to using this knowledge to have more meaningful dialogues with you going forward.\n"
     ]
    }
   ],
   "source": [
    "async for msg in chat(\"I'm building long term memory for you! then you can know me\", user_id, thread_id):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "064c1f70-9c39-45b0-9aa8-e1df506b924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await manager.trigger(user_id=user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbe8f6bd-8cc7-4a78-9e85-970c061b0433",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid I still don't have enough information to make a reasonable guess about what you're doing. Without any additional details from you about the context or activity, I don't have enough to go on. Could you please give me a few more clues or describe the situation you're in? That would really help me engage with your question and try to figure out what you might be up to. I'd be happy to take another guess if you can provide a bit more information.\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 2\"\n",
    "async for msg in chat(\"Hi there. guess what i'm doing?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf9c3ad4-a173-483d-8fed-a4764aa2e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm, I'm afraid I don't have any specific memories about what you've been working on previously. Since I'm an AI assistant without a persistent long-term memory, I don't have details about our past conversations stored. Could you remind me what project or task you've been focused on? I'd be happy to discuss your progress with you, but I need you to provide some more context first. Please feel free to fill me in on what you've been working on.\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"I been working! making some progress\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f296170-51bf-4b7d-aabf-62fdea86d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't actually have any specific memories about what you've been working on previously. As an AI assistant, I don't have a persistent long-term memory of our past conversations. Without you providing more context about what project or task you've been focused on, I don't have anything to \"remember\" or guess about. Could you please give me some more details about what you've been working on? I'd be happy to discuss it with you, but I need you to refresh my memory first. Let me know what you've been focusing your efforts on.\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"Guess what i was working on? Remeber?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2ba7b-7226-441c-b5f3-4b0a34b6702a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62b9fee9-206b-4d4b-a4b4-82ca281f3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I assist you today? I'm happy to help, but just to clarify - as an AI assistant, I don't have a detailed long-term memory of our previous conversations. So if there's something specific you'd like me to recall or continue discussing, please provide me with some additional context. I'm here to help, but I rely on you to fill me in on the details so I can best understand how to assist you. How can I be of help today?\n"
     ]
    }
   ],
   "source": [
    "new_thread = \"convo 3\"\n",
    "async for msg in chat(\"HI?\", user_id, new_thread):\n",
    "    print(msg.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
